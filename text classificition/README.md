# 文本分类器——贝叶斯算法

## 1 简介

贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级自然语言处理模型也可以从它演化而来。因此，学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口。

##2 用机器学习的视角理解贝叶斯公式 

在机器学习的视角下，我们把 X 理解成**“具有某特征”**，把 Y 理解成**“类别标签”**(一般机器学习为题中都是X=>特征, Y=>结果对吧)。在最简单的二分类问题(是与否判定)下，我们将 Y 理解成**“属于某类”**的标签。于是贝叶斯公式就变形成了下面的样子:

>**$P(“属于某类”|“具有某特征”) =\frac{P(“具有某特征”|“属于某类”)P(“属于某类”) }{P(“具有某特征”) }$**

我们简化解释一下上述公式： 

>$P(“属于某类”|“具有某特征”)=$在已知某样本“具有某特征”的条件下，该样本“属于某类”的概率。所以叫做**『后验概率』**。 
>
>$P(“具有某特征”|“属于某类”)=$在已知某样本“属于某类”的条件下，该样本“具有某特征”的概率。  
>
>$P(“属于某类”)=$（在未知某样本具有该“具有某特征”的条件下，）该样本“属于某类”的概率。所以叫做**『先验概率』**。
>
> $P(“具有某特征”)=$(在未知某样本“属于某类”的条件下，)该样本“具有某特征”的概率。 

## 3 条件独立假设

下面我们马上会看到一个非常简单粗暴的假设。 

概率$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|"垃圾邮件"）$依旧不够好求，我们引进一个**很朴素的近似**。为了让公式显得更加紧凑，我们令字母S表示“垃圾邮件”,令字母H表示“正常邮件”。近似公式如下： 

>$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）\\=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S） \\×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)  $

这就是传说中的**条件独立假设**。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。

于是我们就只需要比较以下两个式子的大小：

> $C=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)\\ ×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)\\ C¯=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H) \\×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”)  $

### 3.1 多项式模型：

如果我们考虑重复词语的情况，也就是说，**重复的词语我们视为其出现多次**，直接按条件独立假设的方式推导，则有 

> $P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S）\\ =P(“代开””|S)P(“发票”|S)P(“增值税”|S)P(“发票”|S)P(“正规”|S)P(“发票”|S）\\=P(“代开””|S)P^{3}(“发票”|S)P(“增值税”|S)P(“正规”|S) $ 

   在统计计算$P(“发票”|S)$时，每个被统计的垃圾邮件样本中重复的词语也统计多次。

> $P(“发票”|S )=\frac{每封垃圾邮件中出现“发票”的次数的总和 }{每封垃圾邮件中所有词出现次数（计算重复次数）的总和  }$

你看这个多次出现的结果，出现在概率的指数/次方上，因此这样的模型叫作**多项式模型**。 

### 3.2 伯努利模型

另一种更加简化的方法是**将重复的词语都视为其只出现1次**， 

> $P(（“代开”,“发票”,“增值税”,“发票”,“正规”,“发票”)|S） =  P(“发票”|S)P(“代开””|S)P(“增值税”|S)P(“正规”|S）  $

统计计算P(“词语”|S）P(“词语”|S）时也是如此 

> $P(“发票”|S） = \frac{ 出现“发票”的垃圾邮件的封数 }{每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和  }$

###3.3 平滑技术 

我们来说个问题(中文NLP里问题超级多，哭瞎T_T)，比如在计算以下独立条件假设的概率： 

> $P(（“我”,“司”,“可”,“办理”,“正规发票”)|S)  = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S） $

我们扫描一下训练集，发现**“正规发票”这个词从出现过！！！\*，于是P(“正规发票”|S）=0P(“正规发票”|S）=0...问题严重了，整个概率都变成0了！！！朴素贝叶斯方法面对一堆0，很凄惨地失效了...更残酷的是**这种情况其实很常见**，因为哪怕训练集再大，也可能有覆盖不到的词语。本质上还是**样本数量太少，不满足大数定律，计算出来的概率失真**。为了解决这样的问题，一种分析思路就是直接不考虑这样的词语，但这种方法就相当于默认给P(“正规发票”|S）赋值为1。其实效果不太好，大量的统计信息给浪费掉了。我们进一步分析，既然可以默认赋值为1，为什么不能默认赋值为一个很小的数？这就是平滑技术的基本思路，依旧保持着一贯的作风，`朴实/土`但是`直接而有效`。 

对于伯努利模型，P(“正规发票”|S）的一种平滑算法是： 

> $P(“正规发票”|S） = \frac{出现“正规发票”的垃圾邮件的封数+1 }{每封垃圾邮件中所有词出现次数（出现了只计算一次）的总和+2  }$

对于多项式模型，P(“正规发票”| S）的一种平滑算法是： 

> $P(“发票”|S） = \frac{每封垃圾邮件中出现“发票”的次数的总和+1 }{每封垃圾邮件中所有词出现次数（计算重复次数）的总和+被统计的词表的词语数量  }$

说起来，平滑技术的种类其实非常多，有兴趣的话回头我们专门拉个专题讲讲好了。这里只提一点，就是所有的**平滑技术都是给未出现在训练集中的词语一个估计的概率，而相应地调低其他已经出现的词语的概率**。 

平滑技术是因为数据集太小而产生的现实需求。**如果数据集足够大，平滑技术对结果的影响将会变小。** 

## 4 相关链接

[手写代码](https://blog.csdn.net/kancy110/article/details/72763276)

[相关算法原理](https://liyaguo6.github.io/NLP/text%20classificition/朴素贝叶斯.html)